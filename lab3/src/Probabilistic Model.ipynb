{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "from nltk.grammar import Nonterminal\n",
    "\n",
    "tree_sents = treebank.parsed_sents()\n",
    "\n",
    "# split sentences into training and testing datasets\n",
    "train_ratio = 0.9\n",
    "train_size = int(len(tree_sents) * train_ratio)\n",
    "tree_sents_train, tree_sents_test = tree_sents[:train_size], tree_sents[train_size:]\n",
    "\n",
    "get_productions = lambda tree_sents : [p for sent in tree_sents for p in sent.productions()]\n",
    "\n",
    "productions_train = get_productions(tree_sents_train)\n",
    "productions_test = get_productions(tree_sents_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[S -> NP-SBJ VP .,\n",
       " NP-SBJ -> NP , ADJP ,,\n",
       " NP -> NNP NNP,\n",
       " NNP -> 'Pierre',\n",
       " NNP -> 'Vinken',\n",
       " , -> ',',\n",
       " ADJP -> NP JJ,\n",
       " NP -> CD NNS,\n",
       " CD -> '61',\n",
       " NNS -> 'years',\n",
       " JJ -> 'old',\n",
       " , -> ',',\n",
       " VP -> MD VP,\n",
       " MD -> 'will',\n",
       " VP -> VB NP PP-CLR NP-TMP,\n",
       " VB -> 'join',\n",
       " NP -> DT NN,\n",
       " DT -> 'the',\n",
       " NN -> 'board',\n",
       " PP-CLR -> IN NP,\n",
       " IN -> 'as',\n",
       " NP -> DT JJ NN,\n",
       " DT -> 'a',\n",
       " JJ -> 'nonexecutive',\n",
       " NN -> 'director',\n",
       " NP-TMP -> NNP CD,\n",
       " NNP -> 'Nov.',\n",
       " CD -> '29',\n",
       " . -> '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_sents[0].productions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NP, VP = Nonterminal('NP'), Nonterminal('VP')\n",
    "\n",
    "# define useful predicates for filtering productions\n",
    "is_non_terminal = lambda p : p.is_nonlexical()\n",
    "is_np = lambda p : p.lhs() == NP\n",
    "is_vp = lambda p : p.lhs() == VP\n",
    "\n",
    "is_rec_np = lambda p : is_np(p) and NP in p.rhs()\n",
    "\n",
    "our_filter = lambda p : is_non_terminal(p) and not is_rec_np(p) and not is_vp(p)\n",
    "\n",
    "productions_train = list(filter(our_filter, productions_train))\n",
    "productions_test = list(filter(our_filter, productions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = productions_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9529152105940776"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute coverage\n",
    "unique_productions_train = set(productions_train)\n",
    "unique_productions_test  = set(productions_test)\n",
    "unique_productions = unique_productions_train | unique_productions_test\n",
    "\n",
    "coverage = 1. * len(unique_productions_train) / len(unique_productions)\n",
    "coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.grammar import PCFG, induce_pcfg\n",
    "treebank_prob_grammar = induce_pcfg(Nonterminal('S'), productions_train)\n",
    "\n",
    "# sort productions by probability of appearing in training\n",
    "sorted_prod = sorted(treebank_prob_grammar.productions(), key=lambda p : p.prob(), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def no_check_coverage_at_all(tokens):\n",
    "    pass\n",
    "treebank_prob_grammar.check_coverage = no_check_coverage_at_all\n",
    "treebank_prob_grammar.check_coverage(tags_as_sents_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.parse.viterbi import ViterbiParser\n",
    "parser = ViterbiParser(treebank_prob_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_sents = treebank.tagged_sents()\n",
    "tagged_sents_train, tagged_sents_test = tagged_sents[:train_size], tagged_sents[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_sent_to_tags = lambda tagged_sent : tuple(zip(*tagged_sent))[1]\n",
    "tags_as_sents_train = list(map(tagged_sent_to_tags, tagged_sents_train))\n",
    "tags_as_sents_test = list(map(tagged_sent_to_tags, tagged_sents_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-114-348d8b944a39>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-114-348d8b944a39>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Ã±\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "good_cnt, bad_cnt = 0, 0\n",
    "predictions = []\n",
    "for k, tags_as_sent in enumerate(tags_as_sents_test):\n",
    "    print(k)\n",
    "    try:\n",
    "        predictions.append(list(parser.parse(tags_as_sent)))\n",
    "        good_cnt += 1\n",
    "    except ValueError:\n",
    "        bad_cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sents = nltk.corpus.treebank_chunk.chunked_sents()\n",
    "sents_train, sents_test = sents[:train_size], sents[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ChunkParser(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)] for sent in train_sents]\n",
    "        self.tagger = nltk.TrigramTagger(train_data)\n",
    "    \n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word,pos) in sentence] \n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag) in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tree is too deeply nested to be printed in CoNLL format",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-7d25794d6abd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchunker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mChunkParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msents_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-80-ab63e601261a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_sents)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mChunkParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChunkParserI\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_sents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree2conlltags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_sents\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrigramTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Emanuel\\Anaconda2\\lib\\site-packages\\nltk\\chunk\\util.pyc\u001b[0m in \u001b[0;36mtree2conlltags\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcontents\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchild\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Tree is too deeply nested to be printed in CoNLL format\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m                 \u001b[0mtags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0mprefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"I-\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tree is too deeply nested to be printed in CoNLL format"
     ]
    }
   ],
   "source": [
    "chunker = ChunkParser(sents_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'The', u'DT'),\n",
       " (u'company', u'NN'),\n",
       " (u'forecast', u'VBD'),\n",
       " (u'that', u'IN'),\n",
       " (u'fourth-quarter', u'NN'),\n",
       " (u'income', u'NN'),\n",
       " (u'from', u'IN'),\n",
       " (u'continuing', u'VBG'),\n",
       " (u'operations', u'NNS'),\n",
       " (u'would', u'MD'),\n",
       " (u'be', u'VB'),\n",
       " (u'``', u'``'),\n",
       " (u'significantly', u'RB'),\n",
       " (u\"''\", u\"''\"),\n",
       " (u'lower', u'JJR'),\n",
       " (u'than', u'IN'),\n",
       " (u'a', u'DT'),\n",
       " (u'year', u'NN'),\n",
       " (u'earlier', u'JJR'),\n",
       " (u'.', u'.')]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = sents_test[0].leaves()\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = chunker.parse(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  97.0%\n",
      "    Precision:     91.1%\n",
      "    Recall:        93.7%\n",
      "    F-Measure:     92.4%\n"
     ]
    }
   ],
   "source": [
    "print chunker.evaluate(sents_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.chunk.conlltags2tree?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
