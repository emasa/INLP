{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import itertools\n",
    "\n",
    "# download punkt model for sentence tokenizer\n",
    "# using nltk.download()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from my_nltk.model.counter import build_vocabulary\n",
    "from entropy_functions import clean_text\n",
    "\n",
    "# english corpus\n",
    "with open('../data/en.txt') as corpus_file:\n",
    "    input_text = corpus_file.read()\n",
    "\n",
    "# tokenize\n",
    "# run experiment with data cleaned\n",
    "sents = [word_tokenize(clean_text(sent)) for sent in sent_tokenize(input_text)]\n",
    "# run experiment without cleaning the data\n",
    "#sents = [word_tokenize(sent) for sent in sent_tokenize(input_text)]\n",
    "\n",
    "# create a vocabulary (it could be used for filtering unfrequent words)\n",
    "vocabulary = build_vocabulary(1, itertools.chain(*sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en.txt corpus using 1-gram. H: 10.560 perplexity: 1509.917\n",
      "en.txt corpus using 2-gram. H: 6.064 perplexity: 66.881\n",
      "en.txt corpus using 3-gram. H: 2.106 perplexity: 4.306\n"
     ]
    }
   ],
   "source": [
    "from my_nltk.model.counter import count_ngrams\n",
    "from my_nltk.model.ngram import NgramModel\n",
    "\n",
    "n_gram = {}\n",
    "for n in [1, 2, 3]:\n",
    "    # store models for testing\n",
    "    n_gram[n] = NgramModel(count_ngrams(n, vocabulary, sents))\n",
    "    print(\"{} corpus using {}-gram. H: {:.3f} perplexity: {:.3f}\".format('en.txt', n, n_gram[n].entropy(), n_gram[n].perplexity()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#results with method 2\n",
    "#en.txt corpus using 1-gram. H: 10.560 perplexity: 1509.917\n",
    "#en.txt corpus using 2-gram. H: 5.884 perplexity: 59.038\n",
    "#en.txt corpus using 3-gram. H: 2.182 perplexity: 4.538\n",
    "% reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from entropy_functions import get_tagged_words_from_file, get_words_from_tagged_sents\n",
    "from entropy_functions import get_tagged_sents, clean_tagged_sent\n",
    "from my_nltk.model.counter import build_vocabulary\n",
    "\n",
    "# create one unique sentence from the file\n",
    "#tagged_words = get_tagged_words_from_file('../data/taggedBrown.txt')\n",
    "#tagged_sents = [clean_tagged_sent(tagged_words)]\n",
    "\n",
    "# use sentences from the news category in the brown corpus \n",
    "# run experiment with data cleaned\n",
    "tagged_sents = [clean_tagged_sent(tagged_sent) for tagged_sent in get_tagged_sents()]\n",
    "\n",
    "# run experiment without cleaning the data\n",
    "#tagged_sents = [tagged_sent for tagged_sent in get_tagged_sents()]\n",
    "\n",
    "# define/override words using the cleaned sentences\n",
    "tagged_words = get_words_from_tagged_sents(tagged_sents)\n",
    "\n",
    "# create a vocabulary (it could be used for filtering unfrequent words)\n",
    "vocabulary = build_vocabulary(1, tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', '<s>', 'the'),\n",
       " ('<s>', 'the', 'fulton'),\n",
       " ('<DET>', 'fulton', 'county'),\n",
       " ('<NOUN>', 'county', 'grand'),\n",
       " ('<NOUN>', 'grand', 'jury'),\n",
       " ('<ADJ>', 'jury', 'said'),\n",
       " ('<NOUN>', 'said', 'friday'),\n",
       " ('<VERB>', 'friday', 'an'),\n",
       " ('<NOUN>', 'an', 'investigation'),\n",
       " ('<DET>', 'investigation', 'of'),\n",
       " ('<NOUN>', 'of', 'atlantas'),\n",
       " ('<ADP>', 'atlantas', 'recent'),\n",
       " ('<NOUN>', 'recent', 'primary'),\n",
       " ('<ADJ>', 'primary', 'election'),\n",
       " ('<NOUN>', 'election', 'produced'),\n",
       " ('<NOUN>', 'produced', 'no'),\n",
       " ('<VERB>', 'no', 'evidence'),\n",
       " ('<DET>', 'evidence', 'that'),\n",
       " ('<NOUN>', 'that', 'any'),\n",
       " ('<ADP>', 'any', 'irregularities'),\n",
       " ('<DET>', 'irregularities', 'took'),\n",
       " ('<NOUN>', 'took', 'place'),\n",
       " ('<VERB>', 'place', '</s>'),\n",
       " ('<NOUN>', '</s>', '</s>')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from my_nltk.model.counter import smooth_count_ngrams\n",
    "\n",
    "# playing around with n-grams\n",
    "\n",
    "# number n for creating a n-gram\n",
    "n = 3\n",
    "# number of components to be smoothed. it can take\n",
    "# values in [0, n]\n",
    "smooth = 1\n",
    "cnt = smooth_count_ngrams(n, vocabulary, smooth, [])\n",
    "# show 3-grams for the first sentence \n",
    "cnt.to_ngrams(tagged_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Sentences ratio: 1.000 #Words ratio: 1.000\n",
      "Brown corpus using 3-gram with 0 smoothed components. H: 1.114707 Perplexity: 2.165510\n",
      "Brown corpus using 3-gram with 1 smoothed components. H: 3.416396 Perplexity: 10.676717\n",
      "Brown corpus using 3-gram with 2 smoothed components. H: 7.542192 Perplexity: 186.391521\n",
      "\n",
      "#Sentences ratio: 0.500 #Words ratio: 0.497\n",
      "Brown corpus using 3-gram with 0 smoothed components. H: 0.903587 Perplexity: 1.870712\n",
      "Brown corpus using 3-gram with 1 smoothed components. H: 2.981320 Perplexity: 7.897085\n",
      "Brown corpus using 3-gram with 2 smoothed components. H: 7.140396 Perplexity: 141.082619\n",
      "\n",
      "#Sentences ratio: 0.250 #Words ratio: 0.258\n",
      "Brown corpus using 3-gram with 0 smoothed components. H: 0.792687 Perplexity: 1.732297\n",
      "Brown corpus using 3-gram with 1 smoothed components. H: 2.655754 Perplexity: 6.301754\n",
      "Brown corpus using 3-gram with 2 smoothed components. H: 6.632691 Perplexity: 99.229083\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from my_nltk.model.counter import smooth_count_ngrams\n",
    "from my_nltk.model.ngram import NgramModel\n",
    "\n",
    "import itertools\n",
    "\n",
    "msg = \"{corpus} corpus using {n}-gram with {smooth} smoothed components. H: {entropy:.6f} Perplexity: {perplexity:.6f}\"\n",
    "\n",
    "n=3\n",
    "brown_trigram = {}\n",
    "for corpus_size in [1, 2, 4]:\n",
    "    \n",
    "    # select a proportion of the sentences of the corpus (keep words proportion) \n",
    "    sents_ratio = 1./corpus_size\n",
    "\n",
    "    if len(tagged_sents) == 1:\n",
    "        selected_tagged_sents = [tagged_sents[0][:int(len(tagged_words) * sents_ratio)]]\n",
    "    else:\n",
    "        selected_tagged_sents = tagged_sents[:int(len(tagged_sents) * sents_ratio)]\n",
    "\n",
    "    words_ratio = 1. * len(list(itertools.chain(*selected_tagged_sents))) / len(tagged_words)\n",
    "    print(\"#Sentences ratio: {:.3f} #Words ratio: {:.3f}\".format(sents_ratio, words_ratio))\n",
    "        \n",
    "    for smooth in [0, 1, 2]:        \n",
    "        # store models for testing\n",
    "        brown_trigram[(corpus_size, smooth)] = NgramModel(smooth_count_ngrams(n, vocabulary, \n",
    "                                                                              smooth, selected_tagged_sents))\n",
    "        \n",
    "        print(msg.format(n=n, corpus='Brown', smooth=smooth, \n",
    "                         entropy=brown_trigram[(corpus_size, smooth)].entropy(), \n",
    "                         perplexity=brown_trigram[(corpus_size, smooth)].perplexity()))\n",
    "    \n",
    "    print() # new line"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
